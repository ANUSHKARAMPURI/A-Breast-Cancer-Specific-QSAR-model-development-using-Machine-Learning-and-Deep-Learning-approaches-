{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da66fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from padelpy import from_smiles\n",
    "\n",
    "def calculate_molecular_descriptors(descriptor_list, output_filename='all_descriptors.csv'):\n",
    "    \"\"\"\n",
    "    Calculate molecular descriptors using padelpy from a list of SMILES IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - descriptor_list (list): List of SMILES IDs.\n",
    "    - output_filename (str, optional): Output filename to save the descriptors (default is 'all_descriptors.csv').\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the calculated descriptors to a CSV file.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store all the descriptors\n",
    "    full_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each SMILES ID\n",
    "    for index, smile_id in enumerate(descriptor_list):\n",
    "        print(f\"Calculating descriptors for SMILES ID {index + 1}/{len(descriptor_list)}\")\n",
    "        output_csv = f'{index}.csv'\n",
    "        from_smiles(smile_id, fingerprints=False, output_csv=output_csv, timeout=600)\n",
    "        df = pd.read_csv(output_csv)\n",
    "        full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "\n",
    "    # Save all descriptors to a single CSV file\n",
    "    full_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Descriptors saved to {output_filename}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your list of SMILES IDs\n",
    "    descriptor_list = ['SMILES_ID_1', 'SMILES_ID_2', 'SMILES_ID_3']  # Example SMILES IDs\n",
    "    \n",
    "    # Call the function to calculate descriptors\n",
    "    calculate_molecular_descriptors(descriptor_list, output_filename='all_descriptors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis, boxcox, yeojohnson\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from MultiColumnLabelEncoder import MultiColumnLabelEncoder\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for Ligand-based drug designing and QSAR model development.\n",
    "    \n",
    "    Steps included:\n",
    "    1. Handling missing values\n",
    "    2. Encoding categorical variables\n",
    "    3. Identifying skewness and kurtosis\n",
    "    4. Transforming columns to reduce skewness and kurtosis\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataset with [features] Molecular Descriptors of drugs,  and [target variable] Drug dose response data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Preprocessed dataset ready for model development.\n",
    "    \"\"\"\n",
    "    \n",
    "    # QSAR Model Development Stage 1: Handling Missing Values\n",
    "    print(\"QSAR MODEL DEVELOPMENT STAGE 1: Handling Missing Values\")\n",
    "    def fill_null_with_mean(df):\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "    \n",
    "    fill_null_with_mean(df)\n",
    "    \n",
    "    # Alternative approach to fill missing values with SimpleImputer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    print(\"Using SimpleImputer to fill missing values...\")\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    # QSAR Model Development Stage 2: Encoding Categorical Variables\n",
    "    print(\"QSAR MODEL DEVELOPMENT STAGE 2: Encoding Categorical Variables\")\n",
    "    mcle = MultiColumnLabelEncoder()\n",
    "    df = mcle.fit_transform(df)\n",
    "    \n",
    "    # QSAR Model Development Stage 3: Identifying Skewness\n",
    "    print(\"QSAR MODEL DEVELOPMENT STAGE 3: Identifying Skewness\")\n",
    "    skewness_values = np.apply_along_axis(skew, axis=0, arr=df)\n",
    "    plt.hist(x=skewness_values)\n",
    "    plt.title('Skewness Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    # QSAR Model Development Stage 4: Identifying Kurtosis\n",
    "    print(\"QSAR MODEL DEVELOPMENT STAGE 4: Identifying Kurtosis\")\n",
    "    kurtosis_values = np.apply_along_axis(kurtosis, axis=0, arr=df)\n",
    "    plt.hist(x=kurtosis_values)\n",
    "    plt.title('Kurtosis Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    # QSAR Model Development Stage 5: Transforming Columns\n",
    "    print(\"QSAR MODEL DEVELOPMENT STAGE 5: Transforming Columns to Reduce Skewness and Kurtosis\")\n",
    "    start_column = 12\n",
    "    X = df.iloc[:, start_column:]\n",
    "\n",
    "    # Threshold for identifying anomalies in kurtosis\n",
    "    kurtosis_threshold = 3\n",
    "\n",
    "    columns_to_boxcox = []\n",
    "    columns_to_yeojohnson = []\n",
    "    columns_to_sqrt = []\n",
    "    columns_to_log = []\n",
    "\n",
    "    for i, kurt in enumerate(kurtosis_values[start_column:]):\n",
    "        col_index = start_column + i\n",
    "        if kurt > kurtosis_threshold:\n",
    "            try:\n",
    "                transformed_column, _ = boxcox(X.iloc[:, i] + 1)\n",
    "                X.iloc[:, i] = transformed_column\n",
    "                columns_to_boxcox.append(col_index)\n",
    "            except ValueError:\n",
    "                transformed_column, _ = yeojohnson(X.iloc[:, i] + 1)\n",
    "                X.iloc[:, i] = transformed_column\n",
    "                columns_to_yeojohnson.append(col_index)\n",
    "        elif kurt < kurtosis_threshold:\n",
    "            if np.min(X.iloc[:, i]) >= 0:\n",
    "                transformed_column = np.sqrt(X.iloc[:, i])\n",
    "                X.iloc[:, i] = transformed_column\n",
    "                columns_to_sqrt.append(col_index)\n",
    "            else:\n",
    "                transformed_column = np.log(X.iloc[:, i] - np.min(X.iloc[:, i]) + 1)\n",
    "                X.iloc[:, i] = transformed_column\n",
    "                columns_to_log.append(col_index)\n",
    "\n",
    "    df.iloc[:, start_column:] = X\n",
    "\n",
    "    print(\"Data preprocessing complete. The dataset is ready for QSAR model development.\")\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# processed_df = preprocess_data(df)\n",
    "# print(processed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e869be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def apply_pca_and_kmeans(df, n_components=2, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Apply PCA for dimensionality reduction and K-Means clustering to analyze cluster effectiveness.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the features for clustering.\n",
    "    n_components (int): Number of components for PCA.\n",
    "    n_clusters (int): Number of clusters for K-Means.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with cluster labels and silhouette scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "     print(\"QSAR MODEL DEVELOPMENT STAGE 6: DIMENSIONALITY REDUCTION\")\n",
    "    scaler = StandardScaler()\n",
    "    df_std = scaler.fit_transform(df)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components, random_state=0)\n",
    "    principal_components = pca.fit_transform(df_std)\n",
    "    \n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df['Cluster_Label'] = kmeans.fit_predict(df_std)\n",
    "    \n",
    "    # Calculate silhouette score for each data point\n",
    "    silhouette_scores = silhouette_score(df_std, df['Cluster_Label'])\n",
    "    df['Silhouette_Score'] = silhouette_scores\n",
    "    \n",
    "    return df, principal_components\n",
    "\n",
    "def apply_tsne_and_plot(df, n_components=2):\n",
    "    \"\"\"\n",
    "    Apply t-SNE for dimensionality reduction and plot the results.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the features for t-SNE.\n",
    "    n_components (int): Number of components for t-SNE.\n",
    "\n",
    "    Returns:\n",
    "    array: Transformed data after t-SNE.\n",
    "    \"\"\"\n",
    "    # Feature selection using SelectKBest and f_regression\n",
    "    selector = SelectKBest(score_func=f_regression, k=500)  # Adjust 'k' as needed\n",
    "    X_selected = selector.fit_transform(df, df['continuous_target'])  # Assuming 'continuous_target' is your target variable\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=n_components, random_state=0)\n",
    "    X_tsne = tsne.fit_transform(X_selected)\n",
    "    \n",
    "    # Plot t-SNE results\n",
    "    if n_components == 2:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['continuous_target'], cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title('t-SNE Plot (2D)')\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.show()\n",
    "    elif n_components == 3:\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=df['continuous_target'], cmap='magma')\n",
    "        fig.colorbar(scatter, ax=ax)\n",
    "        ax.set_title('t-SNE Plot (3D)')\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        plt.show()\n",
    "    \n",
    "    return X_tsne\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame with features and 'continuous_target' is your continuous target variable\n",
    "\n",
    "# Apply PCA and K-Means clustering\n",
    "df_clustered, pca_components = apply_pca_and_kmeans(df.drop(columns=['continuous_target']), n_components=2, n_clusters=3)\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_label in range(3):\n",
    "    cluster_data = pca_components[df_clustered['Cluster_Label'] == cluster_label]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f'Cluster {cluster_label + 1}')\n",
    "\n",
    "plt.title('Clustering of Drugs Using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Apply t-SNE and plot\n",
    "tsne_components = apply_tsne_and_plot(df, n_components=2)\n",
    "\n",
    "# Save the DataFrame with cluster labels and silhouette scores to an Excel file\n",
    "df_clustered.to_excel(\"clustered_drugs_with_scores.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c674c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def perform_grid_search(models_list, x, y, param_grid, cv=5):\n",
    "    \"\"\"\n",
    "    Perform Grid Search CV for multiple models to optimize hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    models_list (dict): Dictionary with model names as keys and model instances as values.\n",
    "    x (DataFrame): Feature dataset.\n",
    "    y (Series): Target variable.\n",
    "    param_grid (dict): Dictionary with model names as keys and parameter grids as values.\n",
    "    cv (int): Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with model names as keys and best scores, estimators, and parameters as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== QSAR MODEL DEVELOPMENT STAGE 7: Grid Search CV Results ===\")\n",
    "    \n",
    "    for model_name, model in models_list.items():\n",
    "        print(f\"\\nPerforming Grid Search for {model_name}...\")\n",
    "        # Perform GridSearchCV with cross-validation\n",
    "        grid_search = GridSearchCV(model, param_grid[model_name], cv=cv)\n",
    "        grid_search.fit(x, y)\n",
    "        \n",
    "        # Store the best score, estimator, and parameters\n",
    "        best_score = grid_search.best_score_\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        # Store the results\n",
    "        results[model_name] = {'Best Score': best_score, 'Best Estimator': best_estimator, 'Best Parameters': best_params}\n",
    "        \n",
    "        # Print model evaluation summary\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Best Score: {best_score:.4f}\")\n",
    "        print(f\"Best Estimator: {best_estimator}\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_optimization_results(results):\n",
    "    \"\"\"\n",
    "    Plot optimization results for visual analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Dictionary with model names as keys and best scores, estimators, and parameters as values.\n",
    "    \"\"\"\n",
    "    scores = [result['Best Score'] for result in results.values()]\n",
    "    models = list(results.keys())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=scores)\n",
    "    plt.title(\"Optimization Results for QSAR Models\")\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Best Score\")\n",
    "    plt.show()\n",
    "\n",
    "def validate_model_performance(model, x_train, y_train, x_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Validate model performance by plotting regression and residual plots, and performing 8-fold cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model to be validated.\n",
    "    x_train (DataFrame): Training feature dataset.\n",
    "    y_train (Series): Training target variable.\n",
    "    x_test (DataFrame): Testing feature dataset.\n",
    "    y_test (Series): Testing target variable.\n",
    "    model_name (str): Name of the model for labeling plots and saving files.\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Regression Plot\n",
    "    plt.figure(figsize=(25, 20))\n",
    "    sns.regplot(x=y_test, y=y_pred, scatter_kws={'s': 15}, ci=95)\n",
    "    plt.xlabel(\"Actual Values\", fontsize=30)\n",
    "    plt.ylabel(\"Predicted Values\", fontsize=30)\n",
    "    plt.title(f\"{model_name} Regression Plot - Test Set\", fontsize=30)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_name}_regression_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Residual Plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    sns.residplot(x=y_pred, y=residuals, scatter_kws={'s': 8, 'color': 'purple'})\n",
    "    plt.xlabel(\"Predicted Values\", fontsize=24)\n",
    "    plt.ylabel(\"Residuals\", fontsize=24)\n",
    "    plt.title(f\"{model_name} Residual Plot - Test Set\", fontsize=26)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_name}_residual_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Cross-Validation\n",
    "    kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, x_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "    mse_cv = -cv_scores.mean()\n",
    "    rmse_cv = np.sqrt(mse_cv)\n",
    "    \n",
    "    # Regression Metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    evs = explained_variance_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Metrics for {model_name}:\")\n",
    "    print(\"Mean Squared Error (Test Set):\", mse)\n",
    "    print(\"Root Mean Squared Error (Test Set):\", rmse)\n",
    "    print(\"Mean Absolute Error (Test Set):\", mae)\n",
    "    print(\"R-squared (Test Set):\", r2)\n",
    "    print(\"Explained Variance Score (Test Set):\", evs)\n",
    "    print(\"Cross-Validated MSE (Train Set):\", mse_cv)\n",
    "    print(\"Cross-Validated RMSE (Train Set):\", rmse_cv)\n",
    "\n",
    "def build_and_evaluate_models(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Build and evaluate models using GridSearchCV for QSAR model development, and validate model performance.\n",
    "\n",
    "    Parameters:\n",
    "    x_train (DataFrame): Training feature dataset.\n",
    "    y_train (Series): Training target variable.\n",
    "    x_test (DataFrame): Testing feature dataset.\n",
    "    y_test (Series): Testing target variable.\n",
    "    \"\"\"\n",
    "    # Define models and parameter grid\n",
    "    models_list = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"ElasticNet\": ElasticNet(),\n",
    "        \"SGDRegressor\": SGDRegressor(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(random_state=5),\n",
    "        \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "        \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "        \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "        \"SVR\": SVR(),\n",
    "        \"MLPRegressor\": MLPRegressor(max_iter=500)\n",
    "    }\n",
    "    param_grid = {\n",
    "        \"LinearRegression\": {},\n",
    "        \"ElasticNet\": {'alpha': [0.1, 1], 'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "        \"SGDRegressor\": {'alpha': [0.0001, 0.001, 0.01], 'penalty': ['l2', 'l1', 'elasticnet']},\n",
    "        \"RandomForestRegressor\": {'n_estimators': [100, 200], 'max_depth': [5, 10], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5]},\n",
    "        \"DecisionTreeRegressor\": {'max_depth': [5, 10], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5]},\n",
    "        \"GradientBoostingRegressor\": {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]},\n",
    "        \"KNeighborsRegressor\": {'n_neighbors': [3, 5], 'weights': ['uniform', 'distance']},\n",
    "        \"SVR\": {'C': [0.1, 1], 'kernel': ['linear', 'rbf']},\n",
    "        \"MLPRegressor\": {'hidden_layer_sizes': [(50,), (100,)], 'activation': ['relu', 'tanh'], 'solver': ['adam']}\n",
    "    }\n",
    "\n",
    "    # Perform GridSearchCV for each model\n",
    "    results = perform_grid_search(models_list, x_train, y_train, param_grid, cv=5)\n",
    "\n",
    "    # Plot optimization results\n",
    "    plot_optimization_results(results)\n",
    "\n",
    "    # Select the best estimator with the optimum parameters for each model\n",
    "    best_estimators = {model_name: result['Best Estimator'] for model_name, result in results.items()}\n",
    "\n",
    "    # Evaluate each model and validate performance\n",
    "    for name, model in best_estimators.items():\n",
    "        print(f\"\\n=== Evaluating {name} ===\")\n",
    "        model.fit(x_train, y_train)\n",
    "        validate_model_performance(model, x_train, y_train, x_test, y_test, name)\n",
    "    \n",
    "    # Identify the best model based on validation metrics\n",
    "    best_model_name = max(results, key=lambda name: results[name]['Best Score'])\n",
    "    print(f\"\\nBest Model Based on Validation Scores: {best_model_name}\")\n",
    "\n",
    "# Example usage with your dataset\n",
    "# x_train, x_test, y_train, y_test should be defined appropriately\n",
    "# build_and_evaluate_models(x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37497b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def feature_selection_lasso(X, y, alpha=0.01):\n",
    "     print(\"QSAR MODEL DEVELOPMENT STAGE 8: FEATURE SELECTION\")\n",
    "    \"\"\"\n",
    "    Perform feature selection using Lasso regression.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): DataFrame containing the features.\n",
    "    y (Series): Series containing the target variable.\n",
    "    alpha (float): Regularization strength for Lasso.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with selected features based on Lasso coefficients.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_scaled, y)\n",
    "\n",
    "    selected_features = X.columns[lasso.coef_ != 0]\n",
    "\n",
    "    return X[selected_features]\n",
    "\n",
    "def feature_selection_extra_trees(X, y, n_features_to_select=10):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Extra Trees Regressor.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): DataFrame containing the features.\n",
    "    y (Series): Series containing the target variable.\n",
    "    n_features_to_select (int): Number of features to select.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with selected features based on Extra Trees Regressor.\n",
    "    \"\"\"\n",
    "    model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    rfe = RFE(model, n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "\n",
    "    selected_features = X.columns[rfe.support_]\n",
    "\n",
    "    return X[selected_features]\n",
    "\n",
    "def feature_selection_rfecv(X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination with Cross-Validation (RFECV).\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): DataFrame containing the features.\n",
    "    y (Series): Series containing the target variable.\n",
    "    cv (int): Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with selected features based on RFECV.\n",
    "    \"\"\"\n",
    "    model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    rfecv = RFECV(estimator=model, step=1, cv=cv)\n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    selected_features = X.columns[rfecv.support_]\n",
    "\n",
    "    return X[selected_features]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'X' is your DataFrame of features and 'y' is your target variable\n",
    "\n",
    "# Perform feature selection using Lasso\n",
    "selected_features_lasso = feature_selection_lasso(X, y)\n",
    "\n",
    "# Perform feature selection using Extra Trees Regressor\n",
    "selected_features_extra_trees = feature_selection_extra_trees(X, y)\n",
    "\n",
    "# Perform feature selection using RFECV\n",
    "selected_features_rfecv = feature_selection_rfecv(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, feature_names, plot_type='summary'):\n",
    "     print(\"QSAR MODEL DEVELOPMENT STAGE 9: SHAP (SHAPELY ADDITIVE EXPLAINATIONS) ANALYSIS\")\n",
    "    \"\"\"\n",
    "    Perform SHAP analysis to explain feature impact on the model.\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained model that supports SHAP values.\n",
    "    X (DataFrame): DataFrame containing the features.\n",
    "    feature_names (list): List of feature names.\n",
    "    plot_type (str): Type of SHAP plot to generate ('summary', 'force', 'bar').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    if plot_type == 'summary':\n",
    "        shap.summary_plot(shap_values, X, feature_names=feature_names)\n",
    "        plt.title('SHAP Summary Plot')\n",
    "        plt.savefig('shap_summary_plot.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "    elif plot_type == 'force':\n",
    "        shap.initjs()\n",
    "        shap.force_plot(explainer.expected_value, shap_values, X)\n",
    "    elif plot_type == 'bar':\n",
    "        shap.summary_plot(shap_values, X, plot_type='bar', feature_names=feature_names)\n",
    "        plt.title('SHAP Feature Importance')\n",
    "        plt.savefig('shap_feature_importance.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model' is your trained model, 'X' is your DataFrame of features, and 'feature_names' is a list of feature names\n",
    "\n",
    "# Perform SHAP analysis with summary plot\n",
    "shap_analysis(model, X, feature_names=list(X.columns), plot_type='summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ac4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fa4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904ed07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
